# -*- coding: utf-8 -*-
"""BUS439_TikTok_Scraper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/163Ch3mTlACaCZBsxBC0ErA0qhcPDUHQB

## Download dependencies
"""

pip install -U yt-dlp

"""## Import dependencies"""

import pandas as pd
import numpy as np
import re
import json
import requests
import yt_dlp

"""## Define Functions"""

# ============================================================================
# TIKTOK SCRAPER WITH TRANSCRIPT EXTRACTION
# ============================================================================

def extract_transcript(info: dict, lang_code: str = 'eng-US') -> str:
    """Extract transcript text from subtitles/captions."""
    captions = info.get('automatic_captions') or info.get('subtitles') or {}

    if not captions:
        return None

    # Try preferred language first, then fall back to any English variant
    for lang in ['eng-US', 'en', 'en-US', 'eng']:
        if lang in captions:
            subtitle_list = captions[lang]
            if subtitle_list and isinstance(subtitle_list, list):

                # Try all available formats
                for sub_format in subtitle_list:
                    sub_url = sub_format.get('url')
                    ext = sub_format.get('ext', 'unknown')

                    if sub_url:
                        try:
                            response = requests.get(sub_url, timeout=10)
                            if response.status_code == 200:
                                text = response.text

                                # Parse WebVTT format
                                if ext == 'vtt' or 'WEBVTT' in text[:50]:
                                    lines = []
                                    for line in text.split('\n'):
                                        line = line.strip()
                                        # Skip empty lines, timestamps, headers, and cue identifiers
                                        if (line and
                                            '-->' not in line and
                                            not line.startswith('WEBVTT') and
                                            not line.startswith('NOTE') and
                                            not line.startswith('STYLE') and
                                            not line.replace('-', '').replace(':', '').replace('.', '').isdigit()):
                                            lines.append(line)

                                    if lines:
                                        result = ' '.join(lines).strip()
                                        print(f"  ✓ Transcript extracted! Length: {len(result)} chars")
                                        return result

                                # Try parsing as JSON
                                else:
                                    try:
                                        sub_data = response.json()

                                        # Handle different JSON formats
                                        if 'events' in sub_data:
                                            text_parts = []
                                            for event in sub_data['events']:
                                                if 'segs' in event:
                                                    for seg in event['segs']:
                                                        if 'utf8' in seg:
                                                            text_parts.append(seg['utf8'])
                                            if text_parts:
                                                result = ' '.join(text_parts).strip()
                                                print(f"  ✓ Transcript extracted! Length: {len(result)} chars")
                                                return result

                                        if 'body' in sub_data:
                                            text_parts = []
                                            for item in sub_data['body']:
                                                if 'content' in item:
                                                    text_parts.append(item['content'])
                                            if text_parts:
                                                result = ' '.join(text_parts).strip()
                                                print(f"  ✓ Transcript extracted! Length: {len(result)} chars")
                                                return result

                                    except json.JSONDecodeError:
                                        # Not JSON, try plain text
                                        if text and len(text) > 50:
                                            result = text.strip()
                                            print(f"  ✓ Transcript extracted (plain text)! Length: {len(result)} chars")
                                            return result

                        except Exception as e:
                            print(f"  Error extracting transcript: {str(e)[:80]}")
                            continue

    return None


def scrape_tiktok_videos(urls_file: str) -> pd.DataFrame:
    """Scrape TikTok video data including comments and transcript status."""

    with open(urls_file, 'r') as f:
        urls = [line.strip() for line in f if line.strip()]

    total_urls = len(urls)
    print(f"Starting to scrape {total_urls} TikTok videos...\n")

    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'skip_download': True,
        'getcomments': True,
        'comment_sort': 'top',
        'comment_limit': 10,
        'writesubtitles': True,
        'allsubtitles': True,
        'cookiefile': './tiktok_cookies.txt',
        'ignoreerrors': True,  # Continue on errors
    }

    all_video_data = []
    failed_urls = []

    for idx, url in enumerate(urls, 1):
        try:
            print(f"[{idx}/{total_urls}] Processing: {url[:60]}...")

            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=False)

                if info is None:
                    print(f"  ❌ Failed: No data returned")
                    failed_urls.append({'url': url, 'reason': 'No data returned'})
                    continue

                # Extract actual transcript text
                transcript_text = extract_transcript(info)

                # Extract comments
                comments_list = info.get('comments', [])

                comment_fields = {}
                for i, comment in enumerate(comments_list[:10]):
                    if isinstance(comment, dict):
                        comment_fields[f'comment_{i+1}_text'] = comment.get('text')
                        comment_fields[f'comment_{i+1}_author'] = comment.get('author')
                    else:
                        comment_fields[f'comment_{i+1}_text'] = str(comment)
                        comment_fields[f'comment_{i+1}_author'] = 'N/A'

                # Extract hashtags
                desc = info.get('description', '')
                hashtags = ' '.join(re.findall(r'#\w+', desc)) if desc else None

                video_data = {
                    # Basic info
                    'url': url,
                    'video_id': info.get('id'),
                    'title': info.get('title'),
                    'description': desc,

                    # Engagement metrics
                    'view_count': info.get('view_count'),
                    'like_count': info.get('like_count'),
                    'comment_count_total': info.get('comment_count'),
                    'share_count': info.get('share_count'),  # Often unavailable
                    'repost_count': info.get('repost_count'),  # Sometimes available
                    'favorite_count': info.get('favorite_count'),  # Bookmarks

                    # Creator info
                    'uploader': info.get('uploader'),
                    'uploader_id': info.get('uploader_id'),
                    'uploader_url': info.get('uploader_url'),
                    'channel_id': info.get('channel_id'),
                    'channel_follower_count': info.get('channel_follower_count'),  # Subscriber count
                    'creator': info.get('creator'),
                    'artist': info.get('artist'),

                    # Temporal data
                    'upload_date': info.get('upload_date'),
                    'timestamp': info.get('timestamp'),
                    'duration': info.get('duration'),

                    # Content metadata
                    'hashtags': hashtags,
                    'tags': info.get('tags'),  # May have structured tags
                    'categories': info.get('categories'),
                    'music': info.get('music'),
                    'track': info.get('track'),
                    'album': info.get('album'),

                    # Video properties
                    'width': info.get('width'),
                    'height': info.get('height'),
                    'fps': info.get('fps'),
                    'aspect_ratio': info.get('aspect_ratio'),

                    # Engagement indicators
                    'age_limit': info.get('age_limit'),
                    'is_live': info.get('is_live'),
                    'was_live': info.get('was_live'),
                    'playable_in_embed': info.get('playable_in_embed'),

                    # Other
                    'webpage_url': info.get('webpage_url'),
                    'thumbnail': info.get('thumbnail'),

                    # Transcript
                    'transcript_text': transcript_text,

                    # Comments
                    **comment_fields,
                }
                all_video_data.append(video_data)
                print(f"  ✓ Success")

        except yt_dlp.utils.DownloadError as e:
            error_msg = str(e)
            print(f"  ❌ Failed: {error_msg[:100]}...")

            # Determine the reason
            if "not be comfortable" in error_msg or "Log in" in error_msg:
                reason = "Age-restricted or login required"
            elif "Private" in error_msg:
                reason = "Private video"
            elif "removed" in error_msg or "available" in error_msg:
                reason = "Video removed or unavailable"
            else:
                reason = error_msg[:200]

            failed_urls.append({'url': url, 'reason': reason})
            continue

        except Exception as e:
            print(f"  ❌ Failed: Unexpected error - {str(e)[:100]}")
            failed_urls.append({'url': url, 'reason': f'Unexpected error: {str(e)[:200]}'})
            continue

    # Summary
    print(f"\n{'='*70}")
    print(f"SCRAPING COMPLETE")
    print(f"{'='*70}")
    print(f"✓ Successfully scraped: {len(all_video_data)}/{total_urls} videos")
    print(f"❌ Failed: {len(failed_urls)}/{total_urls} videos")

    if failed_urls:
        print(f"\nFailed URLs saved to: failed_urls.csv")
        pd.DataFrame(failed_urls).to_csv('failed_urls.csv', index=False)

    if not all_video_data:
        print("\n⚠️  WARNING: No videos were successfully scraped!")
        return pd.DataFrame()

    df = pd.DataFrame(all_video_data)
    df['upload_date'] = pd.to_datetime(df['upload_date'], format='%Y%m%d', errors='coerce')

    return df

# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main(urls_file: str = '/content/tiktok_url_dataset.txt'):
    """Main pipeline: scrape TikTok videos and analyze for sponsored content."""

    # Scrape videos
    df = scrape_tiktok_videos(urls_file)

    if df.empty:
        print("\n⚠️  No videos were scraped. Cannot proceed with analysis.")
        return None, None

    return df

"""## Main execution"""

# Usage:
df = main()

print(f"Dropping columns with 100% missing: {df.columns[df.isnull().all()].tolist()}")

df = df.dropna(axis=1, how='all')

df.columns

df

# Create follower count mapping
follower_mapping = {
    'cbum': 23600000,
    'demibagby': 14200000,
    'leanadeeb': 11600000,
    'leanbeefpatty': 8100000,
    'noeldeyzel_bodybuilder': 7500000,
    'carlosbelcast': 5000000,
    'ayoubm_': 4000000,
    'alexeubank2.0': 2800000,
    'senada.greca': 2600000,
    'ulissesworld': 3000000,
    'krissycela': 3300000,
    'squatuniversity': 3300000,
    'fitbyma': 780000,
    'mad_fit': 350000,
    'massy.arias': 371000,
    'fitwithmezz': 307700,
    'thejustinjfit': 120000,
    'sivan.tm': 570000,
    'heidisomers': 535000,
    'ariannedeveau': 85000,
    'forcephysiquefitness': 99500,
    'charleeatkins': 40700,
    'brentonrosssimmons': 24500,
    'jeaniusuwu': 47600,
    'vanessaberimbauu': 99300
}

# Add follower_count column by mapping uploader_id
df['follower_count'] = df['uploader'].map(follower_mapping)

df

df.to_csv('tiktok_data.csv', index=False)

