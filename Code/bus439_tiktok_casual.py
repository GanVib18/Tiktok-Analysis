# -*- coding: utf-8 -*-
"""BUS439_tiktok_casual.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l2GpYRrB1vQoX9WycQGpKkTHhsFUGwCQ
"""

!pip install nltk textstat vaderSentiment empath scikit-learn

# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import statsmodels.api as sm
import statsmodels.formula.api as smf
from scipy.stats import ttest_ind, zscore
from empath import Empath
import nltk
nltk.download('vader_lexicon')
nltk.download('stopwords')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
import textstat
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

# Load Data
df = pd.read_csv('/content/tiktok_data_analyzed.csv')

# Initialize Analyzers
lexicon = Empath()
vader = SentimentIntensityAnalyzer()

# ==========================================
# ðŸŽ¯ PHASE 1: DATA PREPARATION & TOPIC MODELING
# ==========================================
def run_phase_1_data_prep(df):
    print("\n--- PHASE 1: DATA PREPARATION & TOPIC MODELING ---")

    # 1.1 Basic Cleaning
    initial_count = len(df)
    df = df.dropna(subset=['description']).copy()
    print(f"Dropped {initial_count - len(df)} rows with missing descriptions.")

    df['is_sponsored'] = df['is_sponsored'].astype(int)

    # 1.2 Handle Dates
    try:
        df['dt_date'] = pd.to_datetime(df['upload_date'], format='%Y%m%d', errors='coerce')
    except:
        df['dt_date'] = pd.to_datetime(df['upload_date'], errors='coerce')

    # 1.3 KPI: Engagement Rate
    df['engagement_rate'] = (df['like_count'] + df['comment_count_total'] + df['repost_count']) / (df['view_count'] + 1)
    df['log_engagement'] = np.log(df['engagement_rate'] + 0.00001)

    # 1.4 TOPIC MODELING (LDA) - New Addition
    # This controls for the fact that a "Finance" video is naturally more commercial than a "Vlog"
    print("Running LDA Topic Modeling...")

    # Vectorize
    stops = list(stopwords.words('english')) + ['tiktok', 'video', 'shorts', 'fyp']
    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=stops, max_features=1000)
    tf = tf_vectorizer.fit_transform(df['description'])

    # Fit LDA (Assumes 5 dominant topics)
    lda = LatentDirichletAllocation(n_components=5, random_state=42)
    lda.fit(tf)

    # Assign Topic to each row
    topic_values = lda.transform(tf)
    df['topic_id'] = topic_values.argmax(axis=1)
    print("Topics assigned (0-4) to control for content category.")

    return df, tf_vectorizer, lda

# ==========================================
# ðŸŽ¯ PHASE 2: INDEPENDENT FEATURE EXTRACTION
# ==========================================
def run_phase_2_feature_extraction(df):
    print("\n--- PHASE 2: EXTRACTING INDEPENDENT DIMENSIONS ---")

    # Categories
    cats_commercial = ['money', 'business', 'shopping', 'payment', 'work', 'economics']
    cats_warmth = ['warmth', 'family', 'friends', 'positive_emotion', 'trust', 'love']
    tiktok_commercial_slang = ['link in bio', 'use code', 'discount', '#ad', 'partner']

    features = []

    for _, row in df.iterrows():
        text = str(row['description'])
        word_count = len(text.split())

        # A. Sentiment
        sentiment = vader.polarity_scores(text)['compound']

        # B. Readability
        if word_count > 3:
            try:
                readability = textstat.flesch_reading_ease(text)
            except:
                readability = 50
        else:
            readability = 100

        # C. Pronouns (Immediacy)
        fp_count = len(re.findall(r'\b(i|me|my|mine|myself)\b', text.lower()))
        sp_count = len(re.findall(r'\b(you|your|yours|yourself)\b', text.lower()))

        # D. Empath Categories
        empath_res = lexicon.analyze(text, categories=cats_commercial + cats_warmth, normalize=False)

        emp_comm_score = 0
        emp_warmth_score = 0
        if empath_res:
            emp_comm_score = sum(empath_res[c] for c in cats_commercial)
            emp_warmth_score = sum(empath_res[c] for c in cats_warmth)

        # Add slang to commercial
        slang_hits = sum(1 for s in tiktok_commercial_slang if s in text.lower())
        emp_comm_score += slang_hits

        features.append({
            'raw_word_count': word_count,
            'raw_sentiment': sentiment,
            'raw_readability': readability,
            'raw_first_person': fp_count,
            'raw_second_person': sp_count,
            'raw_commercial': emp_comm_score,
            'raw_warmth': emp_warmth_score
        })

    feat_df = pd.DataFrame(features)
    df = pd.concat([df.reset_index(drop=True), feat_df], axis=1)

    # --- NORMALIZE & CREATE RATIOS ---

    # 1. Z-Score Helper
    def get_z(col):
        if col.std() == 0: return col * 0
        return (col - col.mean()) / col.std()

    # 2. Create the Independent Analytical Dimensions
    df['z_warmth'] = get_z(df['raw_warmth'])
    df['z_commercial'] = get_z(df['raw_commercial'])
    df['z_readability'] = get_z(df['raw_readability'])
    df['z_sentiment'] = get_z(df['raw_sentiment'])

    # 3. New Metric: Immediacy (First Person usage standardized)
    df['z_immediacy'] = get_z(df['raw_first_person'] / (df['raw_word_count'] + 1))

    # 4. New Metric: Self-Focus Ratio (Me vs. You)
    # High = Talking about self ("My routine"); Low = Talking to audience ("You should try")
    df['self_focus_ratio'] = df['raw_first_person'] / (df['raw_second_person'] + 1)
    df['z_self_focus'] = get_z(df['self_focus_ratio'])

    print("Feature extraction complete. Dimensions isolated.")
    return df

# ==========================================
# ðŸŽ¯ PHASE 3: TEMPORAL MATCHING
# ==========================================
def run_phase_3_matching(df):
    print("\n--- PHASE 3: MATCHING (TEMPORAL) ---")

    matches = []
    sponsored_subset = df[df['is_sponsored'] == 1]
    organic_subset = df[df['is_sponsored'] == 0]

    for idx, sp_row in sponsored_subset.iterrows():
        candidates = organic_subset[organic_subset['uploader'] == sp_row['uploader']].copy()
        if candidates.empty: continue

        candidates['days_diff'] = (candidates['dt_date'] - sp_row['dt_date']).abs().dt.days
        best_matches = candidates.sort_values('days_diff').head(3)

        matches.append(sp_row.to_dict())
        for _, org_row in best_matches.iterrows():
            r = org_row.to_dict()
            r['match_group_id'] = sp_row['video_id']
            matches.append(r)

    matched_df = pd.DataFrame(matches)
    print(f"Matched Dataset: {len(matched_df)} rows.")
    return matched_df

# ==========================================
# ðŸŽ¯ PHASE 4: MULTIVARIATE REGRESSION LOOP
# ==========================================
def run_phase_4_multivariate_analysis(matched_df):
    print("\n--- PHASE 4: MULTIVARIATE REGRESSION ANALYSIS ---")

    # Predictor Prep
    matched_df['z_followers'] = zscore(np.log(matched_df['follower_count'] + 1))
    matched_df['z_word_count'] = zscore(matched_df['raw_word_count'])

    # List of Independent Dimensions to Test
    dimensions = {
        'Commercialism': 'z_commercial',
        'Warmth': 'z_warmth',
        'Immediacy (First Person)': 'z_immediacy',
        'Self Focus (Me/You)': 'z_self_focus',
        'Readability': 'z_readability'
    }

    results_summary = []

    # Filter for stability
    creator_counts = matched_df['uploader'].value_counts()
    valid_creators = creator_counts[creator_counts >= 4].index
    df_clean = matched_df[matched_df['uploader'].isin(valid_creators)].copy()

    print(f"\n>> Running Mixed Models for {len(dimensions)} dimensions independently...")
    print(f">> Controlling for: Word Count + Topic (Content Category)")

    for name, dep_var in dimensions.items():
        print(f"\n... Testing Dimension: {name.upper()}")

        # Formula: Outcome ~ Sponsor + Size + WordCount + Topic + Random(Creator)
        # We assume Topic ID is categorical (C)
        formula = f"{dep_var} ~ is_sponsored + z_word_count + C(topic_id)"

        try:
            md = smf.mixedlm(formula, df_clean, groups=df_clean["uploader"])
            mdf = md.fit(method='powell', maxiter=500) # Powell is robust

            # Extract Sponsorship Coefficient
            coef = mdf.params['is_sponsored']
            pval = mdf.pvalues['is_sponsored']

            print(f"   -> Effect of Sponsorship: {coef:.4f} (p={pval:.4f})")
            if pval < 0.05: print("   *** SIGNIFICANT ***")

            results_summary.append({
                'Dimension': name,
                'Coef': coef,
                'P-Value': pval,
                'Significant': pval < 0.05
            })

        except Exception as e:
            print(f"   -> Failed to converge: {e}")

    # ============================================
    # MODEL 4.2: CONSEQUENCE ANALYSIS (ENGAGEMENT)
    # ============================================
    print("\n\n--- MODEL 4.2: WHAT DRIVES ENGAGEMENT? (Sponsored Only) ---")
    # We want to know: Which specific trait actually helps ads perform?

    sp_only = df_clean[df_clean['is_sponsored'] == 1].copy()

    # Formula: Engagement ~ The Dimensions + Controls
    # We remove 'z_self_focus' to avoid collinearity with 'z_immediacy'
    eng_formula = ("log_engagement ~ z_commercial + z_warmth + z_immediacy + "
                   "z_readability + z_followers + z_word_count")

    res_eng = smf.ols(eng_formula, data=sp_only).fit(cov_type='HC3')
    print(res_eng.summary())

    return pd.DataFrame(results_summary), df_clean

# ==========================================
# ðŸŽ¯ PHASE 5: VISUALIZATION (UPDATED)
# ==========================================
import matplotlib.pyplot as plt
import seaborn as sns

def create_dimension_plot(results_df):
    if results_df.empty: return

    print("\n--- Generating Independent Effects Plot ---")
    plt.figure(figsize=(10, 6))
    sns.set_style("whitegrid")

    # Color bars by significance (Red=Sig Negative, Green=Sig Positive, Grey=Insignificant)
    colors = []
    for _, row in results_df.iterrows():
        if row['P-Value'] > 0.05:
            colors.append('grey')
        else:
            colors.append('#e74c3c' if row['Coef'] < 0 else '#2ecc71')

    ax = sns.barplot(x='Dimension', y='Coef', data=results_df, palette=colors)

    plt.axhline(0, color='black', linewidth=1)
    plt.title('Impact of Sponsorship on Authenticity Dimensions (Z-Score Shift)', fontsize=14)
    plt.ylabel('Standardized Effect Size (Beta)', fontsize=12)
    plt.xlabel('Linguistic Dimension', fontsize=12)

    # Add labels
    for i, p in enumerate(ax.patches):
        val = results_df.iloc[i]['Coef']
        sig = "***" if results_df.iloc[i]['P-Value'] < 0.001 else "**" if results_df.iloc[i]['P-Value'] < 0.01 else "*" if results_df.iloc[i]['P-Value'] < 0.05 else "ns"
        ax.annotate(f"{val:.2f}\n({sig})",
                    (p.get_x() + p.get_width() / 2., p.get_height()),
                    ha='center', va='center', xytext=(0, 10 if val > 0 else -10),
                    textcoords='offset points')

    plt.tight_layout()
    plt.show()

# ==========================================
# ðŸš€ EXECUTION BLOCK
# ==========================================

# 1. Prep & Topic Modeling
df_clean, tf_vectorizer, lda = run_phase_1_data_prep(df)

# 2. Extract Dimensions
df_features = run_phase_2_feature_extraction(df_clean)

# 3. Match Organic vs Sponsored
df_matched = run_phase_3_matching(df_features)

# 4. Run Multivariate Analysis
results, final_df = run_phase_4_multivariate_analysis(df_matched)

# 5. Visualize
create_dimension_plot(results)

print("\n--- Analysis Complete ---")

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from math import pi

# ==========================================
# ðŸŽ¯ PHASE 6: ADVANCED VISUALS & INTERACTIONS
# ==========================================
def run_phase_6_advanced_visuals(df):
    print("\n--- PHASE 6: ADVANCED VISUALIZATION & INTERACTION ANALYSIS ---")

    # -------------------------------------------------------
    # 6.1 RADAR CHART: The "Linguistic Fingerprint"
    # -------------------------------------------------------
    print("Generating Radar Chart (Sponsorship Shape)...")

    # Select dimensions to plot
    dims = ['z_commercial', 'z_warmth', 'z_immediacy', 'z_readability', 'z_self_focus']
    labels = ['Commercialism', 'Warmth', 'Immediacy', 'Readability', 'Self-Focus']

    # Calculate means by sponsorship status
    means = df.groupby('is_sponsored')[dims].mean()

    # Setup for Radar Chart
    N = len(dims)
    angles = [n / float(N) * 2 * pi for n in range(N)]
    angles += angles[:1] # Close the loop

    plt.figure(figsize=(8, 8))
    ax = plt.subplot(111, polar=True)

    # Helper to plot a specific row
    def plot_radar(row_data, color, label):
        values = row_data.values.flatten().tolist()
        values += values[:1] # Close the loop
        ax.plot(angles, values, linewidth=2, linestyle='solid', label=label, color=color)
        ax.fill(angles, values, color=color, alpha=0.1)

    # Plot Organic (Index 0)
    plot_radar(means.loc[0], '#2ecc71', 'Organic Content')

    # Plot Sponsored (Index 1)
    plot_radar(means.loc[1], '#e74c3c', 'Sponsored Content')

    # Formatting
    plt.xticks(angles[:-1], labels, size=10)
    ax.set_rlabel_position(0)
    plt.yticks([-0.5, 0, 0.5], ["-0.5", "Avg", "0.5"], color="grey", size=7)
    plt.ylim(-1, 1) # Limit Z-scores to viewable range
    plt.title('The "Shape" of Selling Out: \nLinguistic Fingerprint of Ads vs. Organic', size=15, y=1.1)
    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
    plt.show()

# -------------------------------------------------------
    # 6.2 VIOLIN PLOTS: Variance & Consistency (CORRECTED)
    # -------------------------------------------------------
    print("Generating Violin Plots (Consistency Analysis)...")

    # Melt data for Seaborn
    df_melt = df.melt(id_vars=['is_sponsored'],
                      value_vars=dims,
                      var_name='Dimension',
                      value_name='Z_Score')

    # Rename for readability
    label_map = dict(zip(dims, labels))
    df_melt['Dimension'] = df_melt['Dimension'].map(label_map)

    plt.figure(figsize=(12, 6))

    # 1. Capture the plot in a variable 'ax'
    ax = sns.violinplot(x='Dimension', y='Z_Score', hue='is_sponsored',
                   data=df_melt, split=True, inner='quartile',
                   palette={0: "#2ecc71", 1: "#e74c3c"})

    plt.axhline(0, color='black', linestyle='--', linewidth=0.8)
    plt.title('Consistency Check: Do Ads have less variance?', fontsize=14)
    plt.ylim(-5, 5)

    # --- THE FIX FOR THE LEGEND ---
    # Get the colored handles (shapes) from the plot
    handles, _ = ax.get_legend_handles_labels()

    # Re-create the legend explicitly linking the handles to the text
    plt.legend(handles=handles, labels=['Organic', 'Sponsored'],
               title='Content Type', loc='upper right')
    # ------------------------------

    plt.show()

    # -------------------------------------------------------
    # 6.3 INTERACTION ANALYSIS: Does Topic Matter?
    # -------------------------------------------------------
    print("\n>> Checking Interaction Effects (Does Category moderate the effect?)...")

    # We test one key dimension: Warmth
    # Formula: Warmth ~ Sponsor * Topic + Controls
    formula = "z_warmth ~ is_sponsored * C(topic_id) + z_word_count"

    try:
        md = smf.mixedlm(formula, df, groups=df["uploader"])
        mdf = md.fit(method='powell', maxiter=200)

        print("\nInteraction Model Results (Dependent Var: Warmth):")
        print(mdf.summary().tables[1].iloc[:, :4]) # Print Coef and P-values only

        print("\nINTERPRETATION GUIDE:")
        print("1. Look at 'is_sponsored:C(topic_id)[T.X]'.")
        print("2. A significant P-value (<0.05) means this topic reacts DIFFERENTLY to sponsorship than the baseline topic.")
        print("3. Positive Coef = This topic stays warmer than expected when sponsored.")
        print("4. Negative Coef = This topic gets even colder than expected when sponsored.")

    except Exception as e:
        print(f"Interaction model failed: {e}")

# ==========================================
# ðŸš€ UPDATE YOUR EXECUTION BLOCK
# ==========================================
# Add this line at the very end of your script:
run_phase_6_advanced_visuals(final_df)

# ==========================================
# ðŸŽ¯ PHASE 7: LINGUISTIC DEEP DIVE
# ==========================================
def run_phase_7_linguistic_deepdive(df, vectorizer, lda_model):
    print("\n--- PHASE 7: LINGUISTIC DEEP DIVE ---")

    # -------------------------------------------------------
    # 7.1 TOPIC DECODER RING
    # -------------------------------------------------------
    print("\n>> Decoding Topic IDs (What do the topics actually represent?)...")
    feature_names = vectorizer.get_feature_names_out()

    topic_dict = {}

    for topic_idx, topic in enumerate(lda_model.components_):
        # Get top 10 words for this topic
        top_indices = topic.argsort()[:-11:-1]
        top_words = [feature_names[i] for i in top_indices]
        topic_label = f"Topic {topic_idx}: " + ", ".join(top_words[:3])
        topic_dict[topic_idx] = topic_label
        print(f"  Topic {topic_idx}: {', '.join(top_words)}")

    # -------------------------------------------------------
    # 7.2 LOG-ODDS RATIO (The "Word Shift" Plot)
    # -------------------------------------------------------
    print("\n>> Generating Word Shift Graph (Log-Odds Ratio)...")

    # Separate texts
    txt_sponsored = df[df['is_sponsored'] == 1]['description']
    txt_organic = df[df['is_sponsored'] == 0]['description']

    # Re-vectorize just for this comparison (Binary count)
    cv = CountVectorizer(stop_words='english', max_features=1000, min_df=5)
    X_spon = cv.fit_transform(txt_sponsored)
    X_org = cv.transform(txt_organic) # Use same vocab

    # Sum words
    sum_spon = np.array(X_spon.sum(axis=0)).flatten()
    sum_org = np.array(X_org.sum(axis=0)).flatten()
    total_spon = sum_spon.sum()
    total_org = sum_org.sum()

    # Calculate Log Odds (with smoothing +1 to avoid div by zero)
    # Formula: log( (Count_Spon + 1) / Total_Spon ) - log( (Count_Org + 1) / Total_Org )
    vocab = cv.get_feature_names_out()

    log_odds = np.log((sum_spon + 1) / total_spon) - np.log((sum_org + 1) / total_org)

    # Create DataFrame for Plotting
    lo_df = pd.DataFrame({'word': vocab, 'log_odds': log_odds})

    # Get Top 15 "Most Sponsored" and Top 15 "Most Organic"
    top_spon = lo_df.sort_values('log_odds', ascending=False).head(15)
    top_org = lo_df.sort_values('log_odds', ascending=True).head(15)
    plot_df = pd.concat([top_spon, top_org])

    # Plot
    plt.figure(figsize=(10, 8))
    sns.set_style("white")

    # Color logic
    colors = ['#e74c3c' if x > 0 else '#2ecc71' for x in plot_df['log_odds']]

    sns.barplot(x='log_odds', y='word', data=plot_df, palette=colors)
    plt.axvline(0, color='black', linewidth=0.8)

    plt.title('The Vocabulary of Selling Out\n(Log-Odds: Which words distinguish Ads vs. Organic?)', fontsize=14)
    plt.xlabel('â† More Organic (Green)      |      More Sponsored (Red) â†’', fontsize=12)
    plt.ylabel('')
    plt.show()

# ==========================================
# ðŸš€ UPDATE EXECUTION
# ==========================================
# You need to pass the vectorizer and lda object from Phase 1
# Note: You will need to modify Phase 1 slightly to return these objects,
# or just ensure they are available in the global scope.

# Quick fix: Assuming 'tf_vectorizer' and 'lda' are still in memory from Phase 1
run_phase_7_linguistic_deepdive(df_clean, tf_vectorizer, lda)

from collections import Counter

def check_top_brands(df):
    print("\n--- BRAND ANALYSIS ---")

    # Filter for sponsored only
    sponsored_text = df[df['is_sponsored'] == 1]['description'].astype(str).str.lower()

    # Simple heuristic: Look for words following hashtags or "use code"
    # But for a quick check, let's just count frequent Proper Nouns (capitalized in original)
    # We reload original text to keep case sensitivity for this step
    original_text = df[df['is_sponsored'] == 1]['description'].dropna()

    words = []
    for text in original_text:
        # distinct words that are capitalized but not at start of sentence (heuristic)
        # This is a rough approx for brands without a Named Entity Recognition model
        tokens = text.split()
        for i, t in enumerate(tokens):
            if i > 0 and t[0].isupper() and t.isalpha():
                words.append(t.lower())

    # Filter out common stopwords that might be capitalized accidentally
    stops = set(stopwords.words('english')) | {'i', 'the', 'a', 'my', 'gym', 'workout'}
    clean_words = [w for w in words if w not in stops]

    # Count
    counts = Counter(clean_words).most_common(20)

    print("Potential Top Brands/Entities in Sponsored Posts:")
    for word, count in counts:
        print(f"  {word}: {count}")

# Run this quick check
check_top_brands(df)

