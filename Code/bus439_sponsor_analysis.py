# -*- coding: utf-8 -*-
"""BUS439_sponsor_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fADwNUhB5B3ensU33rS6a3qROnAXNtev
"""

import pandas as pd
import re
from collections import Counter
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import warnings
warnings.filterwarnings('ignore')

def analyze_video_performance(file_path="checkpoint_175.csv"):
    """
    Comprehensive analysis of video performance drivers including:
    - Numerical correlations
    - Word frequency analysis
    - Performance distribution
    - Score comparisons across performance levels
    """

    # --- Data Loading ---
    try:
        df = pd.read_csv(file_path)
        print(f"\nSuccessfully loaded {len(df)} videos from {file_path}")
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        print("Please ensure the file exists in the current directory.")
        return
    except Exception as e:
        print(f"Error loading file: {e}")
        return

    print("\n" + "="*70)
    print("              VIDEO PERFORMANCE DRIVERS ANALYSIS")
    print("="*70)

    # --- Data Cleaning: Standardize Performance Labels ---
    if 'expected_performance' in df.columns:
        # Store original for reference
        df['original_performance'] = df['expected_performance']

        # Normalize performance labels
        df['expected_performance'] = df['expected_performance'].str.lower().str.strip()

        # Standardize variations
        performance_mapping = {
            'high': 'high',
            'medium': 'medium',
            'low': 'low',
            'medium-high': 'medium-high',
            'medium high': 'medium-high',
            'medium to high': 'medium-high',
            'medium-to-high': 'medium-high',
            'mediumhigh': 'medium-high',
            'medium-low': 'medium-low',
            'medium low': 'medium-low',
            'medium to low': 'medium-low'
        }

        df['expected_performance'] = df['expected_performance'].map(performance_mapping).fillna(df['expected_performance'])

        print("\nüìä Performance labels standardized:")
        original_unique = df['original_performance'].nunique()
        standardized_unique = df['expected_performance'].nunique()
        print(f"   Original categories: {original_unique} ‚Üí Standardized: {standardized_unique}")

    # --- Data Overview ---
    print("\n## DATA OVERVIEW")
    print("-" * 70)
    print(f"Total Videos: {len(df)}")

    if 'expected_performance' in df.columns:
        perf_counts = df['expected_performance'].value_counts().sort_index()
        print(f"\nPerformance Distribution (Standardized):")
        for level, count in perf_counts.items():
            pct = (count / len(df)) * 100
            bar = '‚ñà' * int(pct / 2)
            print(f"  {level.capitalize():<15} {count:>3} ({pct:>5.1f}%) {bar}")

    # --- 1. Numerical Drivers Analysis ---
    print("\n\n## 1. NUMERICAL DRIVERS ANALYSIS")
    print("-" * 70)

    numeric_cols = ['engagement_rate', 'authenticity_score', 'shareability_score']
    available_cols = [col for col in numeric_cols if col in df.columns]

    if len(available_cols) >= 2:
        try:
            corr_data = df[available_cols].dropna()
            correlation_matrix = corr_data.corr()

            print("\nCorrelation Matrix:")
            print(correlation_matrix.round(3).to_string())

            if 'engagement_rate' in available_cols:
                print("\n### Key Insights:")
                for col in available_cols:
                    if col != 'engagement_rate':
                        corr_val = correlation_matrix.loc['engagement_rate', col]
                        strength = get_correlation_strength(corr_val)
                        direction = "positive" if corr_val > 0 else "negative"
                        print(f"  ‚Ä¢ Engagement ‚Üî {col.replace('_', ' ').title()}: {corr_val:.3f} ({strength} {direction})")

                print("\nüí° Interpretation:")
                print("   Shareability is a MODERATE predictor of engagement (0.426)")
                print("   Authenticity has a WEAK relationship with engagement (0.185)")
                print("   ‚Üí Focus on shareability factors to boost engagement!")

            # Visualize correlation matrix
            plt.figure(figsize=(8, 6))
            sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,
                       fmt='.3f', square=True, linewidths=1, cbar_kws={'label': 'Correlation'})
            plt.title('Correlation Matrix: Performance Metrics', fontsize=14, fontweight='bold', pad=15)
            plt.tight_layout()
            plt.savefig('correlation_matrix.png', dpi=300, bbox_inches='tight')
            print("\n‚úì Saved: correlation_matrix.png")
            plt.close()

        except Exception as e:
            print(f"Could not perform correlation analysis: {e}")
    else:
        print("Insufficient numeric columns for correlation analysis")

    # --- 2. Performance Level Comparison ---
    print("\n\n## 2. PERFORMANCE LEVEL COMPARISON")
    print("-" * 70)

    if 'expected_performance' in df.columns and len(available_cols) > 0:
        try:
            # Define order for visualization
            perf_order = ['low', 'medium-low', 'medium', 'medium-high', 'high']
            available_perfs = [p for p in perf_order if p in df['expected_performance'].values]

            comparison_df = df[df['expected_performance'].isin(available_perfs)].groupby('expected_performance')[available_cols].mean()
            comparison_df = comparison_df.reindex(available_perfs)

            print("\nAverage Scores by Performance Level:")
            print(comparison_df.round(3).to_string())

            # Calculate differences
            if 'high' in available_perfs and 'low' in available_perfs:
                print("\n### High vs Low Performance Gap:")
                for col in available_cols:
                    high_val = comparison_df.loc['high', col]
                    low_val = comparison_df.loc['low', col]
                    diff = high_val - low_val
                    pct_diff = (diff / low_val) * 100
                    print(f"  ‚Ä¢ {col.replace('_', ' ').title()}: +{diff:.2f} ({pct_diff:+.1f}%)")

            # Visualize comparison
            fig, axes = plt.subplots(1, len(available_cols), figsize=(6*len(available_cols), 5))
            if len(available_cols) == 1:
                axes = [axes]

            colors_map = {
                'low': '#e74c3c',
                'medium-low': '#e67e22',
                'medium': '#f39c12',
                'medium-high': '#3498db',
                'high': '#2ecc71'
            }

            for idx, col in enumerate(available_cols):
                plot_data = []
                plot_labels = []
                plot_colors = []

                for perf in available_perfs:
                    data = df[df['expected_performance'] == perf][col].dropna()
                    if len(data) > 0:
                        plot_data.append(data)
                        plot_labels.append(perf.capitalize())
                        plot_colors.append(colors_map.get(perf, '#95a5a6'))

                bp = axes[idx].boxplot(plot_data, tick_labels=plot_labels, patch_artist=True)

                for patch, color in zip(bp['boxes'], plot_colors):
                    patch.set_facecolor(color)
                    patch.set_alpha(0.7)

                # Styling
                for element in ['whiskers', 'fliers', 'means', 'medians', 'caps']:
                    plt.setp(bp[element], color='#2c3e50', linewidth=1.5)

                axes[idx].set_title(col.replace('_', ' ').title(), fontweight='bold', fontsize=12)
                axes[idx].set_ylabel('Score', fontweight='bold')
                axes[idx].grid(axis='y', alpha=0.3, linestyle='--')
                axes[idx].set_xlabel('Performance Level', fontweight='bold')

            plt.suptitle('Score Distribution by Performance Level', fontsize=14, fontweight='bold', y=1.02)
            plt.tight_layout()
            plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')
            print("\n‚úì Saved: performance_comparison.png")
            plt.close()

        except Exception as e:
            print(f"Could not perform performance comparison: {e}")

    # --- 3. Textual Drivers Analysis ---
    print("\n\n## 3. TEXTUAL DRIVERS ANALYSIS (HIGH PERFORMERS)")
    print("-" * 70)

    # Enhanced stop words
    stop_words = set((
        'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're",
        "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he',
        'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's",
        'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',
        'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are',
        'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',
        'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',
        'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',
        'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from',
        'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',
        'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',
        'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor',
        'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can',
        'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm',
        'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn',
        "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven',
        "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't",
        'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't",
        'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"
    ))

    # Domain-specific noise words
    noise_words = [
        'video', 'content', 'audience', 'brand', 'sponsor', 'rate', 'high',
        'performance', 'integration', 'effectiveness', 'suggests', 'makes',
        'relativity', 'strong', 'low', 'count', 'view', 'views', 'viewership',
        'reason', 'score', 'indicates', 'approval', 'ad', 'could', 'one',
        'would', 'also', 'though', 'well', 'get', 'use', 'using', 'way',
        'via', 'see', 'story', 'likely', 'potential', 'product', 'message',
        'help', 'feel', 'commercial', 'lack', 'even', 'make', 'level', 'might',
        'seem', 'may', 'however', 'lot', 'many', 'much', 'around', 'somewhat',
        'relatively', 'overall', 'general', 'particular', 'given', 'due', 'thus',
        'therefore', 'although', 'still', 'rather', 'quite', 'per', 'mainly'
    ]
    stop_words.update(noise_words)

    text_columns = ['performance_drivers', 'audience_reason']
    available_text_cols = [col for col in text_columns if col in df.columns]

    if 'expected_performance' in df.columns and available_text_cols:
        try:
            high_performance_df = df[df['expected_performance'] == 'high'].copy()

            if len(high_performance_df) > 0:
                # Combine text from available columns - FIXED
                all_texts = []
                for col in available_text_cols:
                    texts = high_performance_df[col].fillna('').astype(str).tolist()
                    all_texts.extend(texts)

                text_data = ' '.join(all_texts)

                # Clean and tokenize
                cleaned_text = re.sub(r'[^a-z\s]', ' ', text_data.lower())
                words = cleaned_text.split()
                filtered_words = [word for word in words if word not in stop_words and len(word) > 2]

                # Count word frequencies
                word_counts = Counter(filtered_words)
                top_words = word_counts.most_common(20)

                print(f"\nTop 20 Themes/Drivers (from {len(high_performance_df)} high-performing videos):\n")
                print(f"{'Rank':<6} {'Theme/Driver':<20} {'Frequency':<12} {'%':<8}")
                print("-" * 50)
                total_words = sum(word_counts.values())
                for rank, (word, freq) in enumerate(top_words, 1):
                    pct = (freq / total_words) * 100
                    print(f"{rank:<6} {word:<20} {freq:<12} {pct:.2f}%")

                # Visualize top drivers
                top_words_df = pd.DataFrame(top_words[:12], columns=['Word', 'Frequency'])

                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

                # Bar chart
                colors_gradient = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_words_df)))
                bars = ax1.barh(top_words_df['Word'], top_words_df['Frequency'], color=colors_gradient)
                ax1.set_xlabel('Frequency of Mention', fontsize=12, fontweight='bold')
                ax1.set_ylabel('Key Theme/Driver', fontsize=12, fontweight='bold')
                ax1.set_title('Top 12 Drivers for High-Performing Videos', fontsize=13, fontweight='bold')
                ax1.invert_yaxis()

                # Add value labels
                for bar in bars:
                    width = bar.get_width()
                    ax1.text(width, bar.get_y() + bar.get_height()/2,
                            f' {int(width)}', ha='left', va='center', fontweight='bold', fontsize=9)

                ax1.grid(axis='x', alpha=0.3, linestyle='--')

                # Word cloud style visualization (size-based)
                top_15 = top_words[:15]
                word_freq_normalized = [(word, freq/max(dict(top_15).values())) for word, freq in top_15]

                ax2.axis('off')
                ax2.set_xlim(0, 10)
                ax2.set_ylim(0, 10)

                positions = [
                    (5, 8), (2, 7), (7.5, 7), (1, 5.5), (5, 5.5), (8.5, 5.5),
                    (2.5, 4), (6.5, 4), (1, 2.5), (4, 2.5), (7, 2.5), (9, 2.5),
                    (2, 1), (5.5, 1), (8.5, 1)
                ]

                for idx, ((word, norm_freq), pos) in enumerate(zip(word_freq_normalized, positions)):
                    size = 10 + (norm_freq * 30)
                    alpha = 0.5 + (norm_freq * 0.5)
                    ax2.text(pos[0], pos[1], word, fontsize=size, ha='center', va='center',
                            fontweight='bold', alpha=alpha, color=colors_gradient[idx % len(colors_gradient)])

                ax2.set_title('Word Cloud: Performance Drivers', fontsize=13, fontweight='bold', pad=10)

                plt.tight_layout()
                plt.savefig('top_performance_drivers.png', dpi=300, bbox_inches='tight')
                print("\n‚úì Saved: top_performance_drivers.png")
                plt.close()

                # Additional insight: Compare high vs low performers
                low_performance_df = df[df['expected_performance'] == 'low'].copy()
                if len(low_performance_df) > 0:
                    print("\n### Comparison: High vs Low Performers")

                    # Get low performer words
                    all_texts_low = []
                    for col in available_text_cols:
                        texts = low_performance_df[col].fillna('').astype(str).tolist()
                        all_texts_low.extend(texts)

                    text_data_low = ' '.join(all_texts_low)
                    cleaned_text_low = re.sub(r'[^a-z\s]', ' ', text_data_low.lower())
                    words_low = cleaned_text_low.split()
                    filtered_words_low = [word for word in words_low if word not in stop_words and len(word) > 2]
                    word_counts_low = Counter(filtered_words_low)

                    # Find words more common in high performers
                    high_words = dict(word_counts.most_common(30))
                    low_words = dict(word_counts_low.most_common(30))

                    print("\nüîç Distinctive themes in HIGH performers:")
                    distinctive_high = []
                    for word in high_words:
                        if word in low_words:
                            ratio = high_words[word] / (low_words[word] + 1)
                        else:
                            ratio = high_words[word]
                        if ratio > 1.5:
                            distinctive_high.append((word, high_words[word], ratio))

                    distinctive_high.sort(key=lambda x: x[2], reverse=True)
                    for word, freq, ratio in distinctive_high[:8]:
                        print(f"   ‚Ä¢ {word:<15} (appears {freq} times, {ratio:.1f}x more than low)")

            else:
                print("No high-performing videos found in dataset")

        except Exception as e:
            print(f"Could not perform textual analysis: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("Required columns for textual analysis not found")

    # --- 4. Engagement Rate Distribution ---
    print("\n\n## 4. ENGAGEMENT RATE DISTRIBUTION")
    print("-" * 70)

    if 'engagement_rate' in df.columns:
        try:
            eng_stats = df['engagement_rate'].describe()
            print("\nEngagement Rate Statistics:")
            print(f"  Mean:     {eng_stats['mean']:.4f}")
            print(f"  Median:   {eng_stats['50%']:.4f}")
            print(f"  Std Dev:  {eng_stats['std']:.4f}")
            print(f"  Min:      {eng_stats['min']:.4f}")
            print(f"  Max:      {eng_stats['max']:.4f}")
            print(f"  Range:    {eng_stats['max'] - eng_stats['min']:.4f}")

            # Percentiles
            print(f"\n  25th percentile: {eng_stats['25%']:.4f}")
            print(f"  75th percentile: {eng_stats['75%']:.4f}")
            print(f"  IQR:             {eng_stats['75%'] - eng_stats['25%']:.4f}")

            # Distribution plot
            fig, axes = plt.subplots(1, 2, figsize=(14, 5))

            # Histogram with KDE
            axes[0].hist(df['engagement_rate'].dropna(), bins=30, color='#3498db',
                        alpha=0.6, edgecolor='black', density=True, label='Distribution')

            # Add KDE
            from scipy import stats
            data_clean = df['engagement_rate'].dropna()
            kde = stats.gaussian_kde(data_clean)
            x_range = np.linspace(data_clean.min(), data_clean.max(), 100)
            axes[0].plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')

            axes[0].axvline(eng_stats['mean'], color='darkred', linestyle='--',
                          linewidth=2, label=f"Mean: {eng_stats['mean']:.2f}")
            axes[0].axvline(eng_stats['50%'], color='darkgreen', linestyle='--',
                          linewidth=2, label=f"Median: {eng_stats['50%']:.2f}")
            axes[0].set_xlabel('Engagement Rate', fontweight='bold')
            axes[0].set_ylabel('Density', fontweight='bold')
            axes[0].set_title('Engagement Rate Distribution', fontweight='bold', fontsize=12)
            axes[0].legend()
            axes[0].grid(alpha=0.3, linestyle='--')

            # Box plot by performance level
            if 'expected_performance' in df.columns:
                perf_order = ['low', 'medium-low', 'medium', 'medium-high', 'high']
                available_perfs = [p for p in perf_order if p in df['expected_performance'].values]

                data_by_perf = [df[df['expected_performance'] == level]['engagement_rate'].dropna()
                               for level in available_perfs]

                colors_map = {
                    'low': '#e74c3c',
                    'medium-low': '#e67e22',
                    'medium': '#f39c12',
                    'medium-high': '#3498db',
                    'high': '#2ecc71'
                }

                bp = axes[1].boxplot(data_by_perf, tick_labels=[p.capitalize() for p in available_perfs],
                                    patch_artist=True, showmeans=True)

                for patch, level in zip(bp['boxes'], available_perfs):
                    patch.set_facecolor(colors_map.get(level, '#95a5a6'))
                    patch.set_alpha(0.7)

                # Style elements
                for element in ['whiskers', 'fliers', 'caps']:
                    plt.setp(bp[element], color='#2c3e50', linewidth=1.5)
                plt.setp(bp['medians'], color='darkblue', linewidth=2)
                plt.setp(bp['means'], marker='D', markerfacecolor='red', markersize=6)

                axes[1].set_ylabel('Engagement Rate', fontweight='bold')
                axes[1].set_xlabel('Performance Level', fontweight='bold')
                axes[1].set_title('Engagement by Performance Level', fontweight='bold', fontsize=12)
                axes[1].grid(axis='y', alpha=0.3, linestyle='--')

            plt.tight_layout()
            plt.savefig('engagement_distribution.png', dpi=300, bbox_inches='tight')
            print("\n‚úì Saved: engagement_distribution.png")
            plt.close()

        except Exception as e:
            print(f"Could not analyze engagement distribution: {e}")

    # --- 5. Top Performers Deep Dive ---
    print("\n\n## 5. TOP PERFORMERS SPOTLIGHT")
    print("-" * 70)

    if 'engagement_rate' in df.columns:
        try:
            # Get top 10 videos
            top_10 = df.nlargest(10, 'engagement_rate')[['engagement_rate', 'expected_performance'] + available_cols]

            print("\nTop 10 Videos by Engagement Rate:")
            print(top_10.to_string(index=True))

            # Count how many top performers are in each category
            if 'expected_performance' in df.columns:
                print("\n### Performance Category Distribution in Top 10:")
                top_10_dist = top_10['expected_performance'].value_counts()
                for perf, count in top_10_dist.items():
                    print(f"  {perf.capitalize():<15} {count} video(s)")

        except Exception as e:
            print(f"Could not generate top performers analysis: {e}")

    # --- Summary ---
    print("\n\n" + "="*70)
    print("                        ANALYSIS COMPLETE")
    print("="*70)
    print("\nüìä Generated visualizations:")
    print("  1. correlation_matrix.png - Relationships between metrics")
    print("  2. performance_comparison.png - Score distributions by level")
    print("  3. top_performance_drivers.png - Key themes in high performers")
    print("  4. engagement_distribution.png - Engagement patterns")

    print("\nüí° Key Takeaways:")
    print("  ‚Ä¢ Shareability is the strongest predictor of engagement")
    print("  ‚Ä¢ Check textual analysis for content themes that resonate")
    print("  ‚Ä¢ Review top performers for patterns to replicate")
    print("\n" + "="*70 + "\n")


def get_correlation_strength(corr_val):
    """Return a description of correlation strength"""
    abs_corr = abs(corr_val)
    if abs_corr >= 0.7:
        return "Strong"
    elif abs_corr >= 0.4:
        return "Moderate"
    elif abs_corr >= 0.2:
        return "Weak"
    else:
        return "Very Weak"


if __name__ == "__main__":
    analyze_video_performance()

import pandas as pd
import re
from collections import Counter
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import warnings
warnings.filterwarnings('ignore')

def analyze_video_performance(file_path="checkpoint_175.csv"):
    """
    Analyzes what makes sponsored videos perform well by examining:
    - Performance drivers vs actual outcomes
    - Authenticity vs shareability trade-offs
    - Specific content strategies that work
    """

    # --- Data Loading ---
    try:
        df = pd.read_csv(file_path)
        print(f"\nSuccessfully loaded {len(df)} videos from {file_path}")
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}")
        return
    except Exception as e:
        print(f"Error loading file: {e}")
        return

    print("\n" + "="*80)
    print("           WHAT MAKES A SPONSORED VIDEO PERFORM WELL?")
    print("="*80)

    # --- Standardize Performance Labels ---
    if 'expected_performance' in df.columns:
        df['original_performance'] = df['expected_performance']
        df['expected_performance'] = df['expected_performance'].str.lower().str.strip()

        performance_mapping = {
            'high': 'high', 'medium': 'medium', 'low': 'low',
            'medium-high': 'medium-high', 'medium high': 'medium-high',
            'medium to high': 'medium-high', 'medium-to-high': 'medium-high',
            'mediumhigh': 'medium-high', 'medium-low': 'medium-low',
            'medium low': 'medium-low', 'medium to low': 'medium-low'
        }
        df['expected_performance'] = df['expected_performance'].map(performance_mapping).fillna(df['expected_performance'])

    # Create binary high performer flag
    df['is_high_performer'] = df['expected_performance'] == 'high'

    print("\n## EXECUTIVE SUMMARY")
    print("-" * 80)
    high_count = df['is_high_performer'].sum()
    high_pct = (high_count / len(df)) * 100
    print(f"üìä Dataset: {len(df)} sponsored videos | {high_count} high performers ({high_pct:.1f}%)")

    if 'engagement_rate' in df.columns:
        avg_engagement_all = df['engagement_rate'].mean()
        avg_engagement_high = df[df['is_high_performer']]['engagement_rate'].mean()
        avg_engagement_low = df[df['expected_performance'] == 'low']['engagement_rate'].mean()

        print(f"\nüéØ Engagement Metrics:")
        print(f"   ‚Ä¢ Average (all videos):      {avg_engagement_all:.2f}%")
        print(f"   ‚Ä¢ High performers:           {avg_engagement_high:.2f}% ‚¨ÜÔ∏è +{avg_engagement_high - avg_engagement_all:.2f}%")
        print(f"   ‚Ä¢ Low performers:            {avg_engagement_low:.2f}% ‚¨áÔ∏è {avg_engagement_low - avg_engagement_all:.2f}%")
        print(f"   ‚Ä¢ High vs Low gap:           {avg_engagement_high - avg_engagement_low:.2f}% ({((avg_engagement_high/avg_engagement_low - 1)*100):.0f}% better)")

    # --- QUESTION 1: What Score Combinations Work Best? ---
    print("\n\n" + "="*80)
    print("## Q1: WHAT SCORE COMBINATIONS DRIVE HIGH PERFORMANCE?")
    print("="*80)

    if all(col in df.columns for col in ['authenticity_score', 'shareability_score']):
        # Create score segments
        df['authenticity_level'] = pd.cut(df['authenticity_score'], bins=[0, 2.5, 3.5, 5],
                                          labels=['Low (1-2)', 'Medium (3)', 'High (4-5)'])
        df['shareability_level'] = pd.cut(df['shareability_score'], bins=[0, 2.5, 3.5, 5],
                                          labels=['Low (1-2)', 'Medium (3)', 'High (4-5)'])

        # Cross-tabulation
        print("\n### Performance by Score Combination:")
        print("\nüìä Average Engagement Rate by Authenticity √ó Shareability:\n")

        pivot_engagement = df.pivot_table(values='engagement_rate',
                                         index='authenticity_level',
                                         columns='shareability_level',
                                         aggfunc='mean')
        print(pivot_engagement.round(2).to_string())

        print("\n\nüìä Count of High Performers by Score Combination:\n")
        pivot_count = df[df['is_high_performer']].pivot_table(values='is_high_performer',
                                                               index='authenticity_level',
                                                               columns='shareability_level',
                                                               aggfunc='count',
                                                               fill_value=0)
        print(pivot_count.astype(int).to_string())

        # Success rate by combination
        print("\n\nüìä Success Rate (% that are high performers) by Score Combination:\n")
        total_by_combo = df.pivot_table(values='is_high_performer',
                                       index='authenticity_level',
                                       columns='shareability_level',
                                       aggfunc='count',
                                       fill_value=0)
        high_by_combo = df[df['is_high_performer']].pivot_table(values='is_high_performer',
                                                                 index='authenticity_level',
                                                                 columns='shareability_level',
                                                                 aggfunc='count',
                                                                 fill_value=0)
        success_rate = (high_by_combo / total_by_combo * 100).fillna(0)
        print(success_rate.round(1).to_string())

        # Key insight
        print("\nüí° KEY INSIGHT:")
        best_combo = success_rate.stack().idxmax()
        best_rate = success_rate.stack().max()
        print(f"   ‚úì Best combination: {best_combo[0]} Authenticity + {best_combo[1]} Shareability")
        print(f"   ‚úì Success rate: {best_rate:.1f}% of videos with this combo are high performers")

        # Visualize
        fig, axes = plt.subplots(1, 2, figsize=(16, 6))

        # Heatmap of engagement
        sns.heatmap(pivot_engagement, annot=True, fmt='.2f', cmap='RdYlGn',
                   ax=axes[0], cbar_kws={'label': 'Avg Engagement %'}, vmin=0, vmax=12)
        axes[0].set_title('Average Engagement by Score Combination', fontweight='bold', fontsize=13)
        axes[0].set_xlabel('Shareability Score', fontweight='bold')
        axes[0].set_ylabel('Authenticity Score', fontweight='bold')

        # Heatmap of success rate
        sns.heatmap(success_rate, annot=True, fmt='.1f', cmap='Blues',
                   ax=axes[1], cbar_kws={'label': 'Success Rate %'}, vmin=0, vmax=100)
        axes[1].set_title('Success Rate (% High Performers) by Score Combination',
                         fontweight='bold', fontsize=13)
        axes[1].set_xlabel('Shareability Score', fontweight='bold')
        axes[1].set_ylabel('Authenticity Score', fontweight='bold')

        plt.tight_layout()
        plt.savefig('score_combination_analysis.png', dpi=300, bbox_inches='tight')
        print("\n‚úì Saved: score_combination_analysis.png")
        plt.close()

    # --- QUESTION 2: Which Content Strategies Actually Work? ---
    print("\n\n" + "="*80)
    print("## Q2: WHICH CONTENT STRATEGIES ACTUALLY DRIVE PERFORMANCE?")
    print("="*80)

    if 'performance_drivers' in df.columns:
        print("\n### Analyzing 'performance_drivers' column for patterns...\n")

        # Define strategy keywords to look for
        strategies = {
            'Authenticity': ['authentic', 'genuine', 'honest', 'real', 'transparent', 'credible'],
            'Relatability': ['relatable', 'personal', 'story', 'experience', 'everyday'],
            'Emotional Connection': ['emotional', 'connection', 'resonates', 'feel', 'empathy'],
            'Clear Value': ['clear', 'value', 'benefit', 'useful', 'practical', 'helpful'],
            'Entertainment': ['entertaining', 'fun', 'humor', 'engaging', 'enjoyable'],
            'Creator Appeal': ['creator', 'influencer', 'personality', 'charisma', 'popular'],
            'Product Fit': ['natural', 'seamless', 'integration', 'fit', 'relevant'],
            'Strong Hook': ['hook', 'opening', 'attention', 'grab', 'compelling'],
            'Call-to-Action': ['cta', 'call to action', 'discount', 'code', 'offer', 'promo'],
            'Target Audience': ['target', 'audience', 'demographic', 'niche', 'segment'],
            'Educational': ['educational', 'informative', 'teaches', 'explains', 'demonstrates'],
            'Aspirational': ['aspirational', 'inspire', 'motivate', 'goal', 'achieve']
        }

        # Count strategy mentions in high vs low performers
        high_texts = ' '.join(df[df['is_high_performer']]['performance_drivers'].fillna('').astype(str).str.lower())
        low_texts = ' '.join(df[df['expected_performance'] == 'low']['performance_drivers'].fillna('').astype(str).str.lower())
        all_texts = ' '.join(df['performance_drivers'].fillna('').astype(str).str.lower())

        strategy_scores = []

        for strategy, keywords in strategies.items():
            high_mentions = sum(high_texts.count(kw) for kw in keywords)
            low_mentions = sum(low_texts.count(kw) for kw in keywords)
            all_mentions = sum(all_texts.count(kw) for kw in keywords)

            # Normalize by number of videos
            high_rate = high_mentions / df['is_high_performer'].sum()
            low_rate = low_mentions / (df['expected_performance'] == 'low').sum() if (df['expected_performance'] == 'low').sum() > 0 else 0.01

            impact_ratio = high_rate / low_rate if low_rate > 0 else high_rate

            strategy_scores.append({
                'Strategy': strategy,
                'High_Mentions': high_mentions,
                'Low_Mentions': low_mentions,
                'High_Rate': high_rate,
                'Low_Rate': low_rate,
                'Impact_Ratio': impact_ratio,
                'Total_Mentions': all_mentions
            })

        strategy_df = pd.DataFrame(strategy_scores).sort_values('Impact_Ratio', ascending=False)

        print("üìä Strategy Impact Analysis (sorted by effectiveness):\n")
        print(f"{'Strategy':<25} {'High Perf':<12} {'Low Perf':<12} {'Impact Ratio':<15} {'Effectiveness'}")
        print("-" * 80)

        for _, row in strategy_df.iterrows():
            effectiveness = "üî•üî•üî•" if row['Impact_Ratio'] > 3 else "üî•üî•" if row['Impact_Ratio'] > 1.5 else "üî•" if row['Impact_Ratio'] > 1 else "‚ö†Ô∏è"
            print(f"{row['Strategy']:<25} {row['High_Mentions']:<12.0f} {row['Low_Mentions']:<12.0f} {row['Impact_Ratio']:<15.2f} {effectiveness}")

        # Visualize strategy impact
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

        # Impact ratio chart
        top_strategies = strategy_df.head(10)
        colors = ['#2ecc71' if x > 2 else '#f39c12' if x > 1.2 else '#e74c3c'
                 for x in top_strategies['Impact_Ratio']]

        ax1.barh(top_strategies['Strategy'], top_strategies['Impact_Ratio'], color=colors, alpha=0.8)
        ax1.axvline(x=1, color='red', linestyle='--', linewidth=2, label='Neutral (1.0x)')
        ax1.set_xlabel('Impact Ratio (High/Low Mention Rate)', fontweight='bold', fontsize=11)
        ax1.set_title('Top 10 Most Effective Content Strategies', fontweight='bold', fontsize=13)
        ax1.legend()
        ax1.grid(axis='x', alpha=0.3)
        ax1.invert_yaxis()

        # Add value labels
        for i, (idx, row) in enumerate(top_strategies.iterrows()):
            ax1.text(row['Impact_Ratio'], i, f"  {row['Impact_Ratio']:.2f}x",
                    va='center', fontweight='bold', fontsize=9)

        # Comparison chart
        top_5 = strategy_df.head(5)
        x = np.arange(len(top_5))
        width = 0.35

        bars1 = ax2.bar(x - width/2, top_5['High_Rate'], width, label='High Performers',
                       color='#2ecc71', alpha=0.8)
        bars2 = ax2.bar(x + width/2, top_5['Low_Rate'], width, label='Low Performers',
                       color='#e74c3c', alpha=0.8)

        ax2.set_xlabel('Strategy', fontweight='bold', fontsize=11)
        ax2.set_ylabel('Mentions per Video', fontweight='bold', fontsize=11)
        ax2.set_title('Top 5 Strategies: High vs Low Performers', fontweight='bold', fontsize=13)
        ax2.set_xticks(x)
        ax2.set_xticklabels(top_5['Strategy'], rotation=45, ha='right')
        ax2.legend()
        ax2.grid(axis='y', alpha=0.3)

        plt.tight_layout()
        plt.savefig('content_strategy_effectiveness.png', dpi=300, bbox_inches='tight')
        print("\n‚úì Saved: content_strategy_effectiveness.png")
        plt.close()

        # Top recommendations
        print("\n\nüí° TOP RECOMMENDATIONS:")
        top_3 = strategy_df.head(3)
        for i, (_, row) in enumerate(top_3.iterrows(), 1):
            print(f"   {i}. Focus on {row['Strategy'].upper()}")
            print(f"      ‚Üí {row['Impact_Ratio']:.1f}x more likely to appear in high performers")
            print(f"      ‚Üí Appears {row['High_Mentions']:.0f} times in top videos vs {row['Low_Mentions']:.0f} in poor ones")

    # --- QUESTION 3: What Do High Performers Talk About? ---
    print("\n\n" + "="*80)
    print("## Q3: WHAT SPECIFIC THEMES APPEAR IN HIGH-PERFORMING CONTENT?")
    print("="*80)

    stop_words = set((
        'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're",
        "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he',
        'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's",
        'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',
        'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are',
        'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',
        'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',
        'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between',
        'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from',
        'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',
        'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any',
        'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor',
        'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can',
        'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm',
        'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn',
        "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven',
        "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't",
        'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't",
        'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"
    ))

    noise_words = [
        'video', 'content', 'audience', 'brand', 'sponsor', 'rate', 'high',
        'performance', 'integration', 'effectiveness', 'suggests', 'makes',
        'strong', 'low', 'count', 'view', 'views', 'viewership', 'reason',
        'score', 'indicates', 'approval', 'ad', 'could', 'one', 'would',
        'also', 'though', 'well', 'get', 'use', 'using', 'way', 'via',
        'see', 'story', 'likely', 'potential', 'message', 'help', 'feel',
        'commercial', 'lack', 'even', 'make', 'level', 'might', 'seem',
        'may', 'however', 'lot', 'many', 'much', 'around', 'somewhat',
        'relatively', 'overall', 'general', 'particular', 'given', 'due'
    ]
    stop_words.update(noise_words)

    text_columns = ['performance_drivers', 'audience_reason']
    available_text_cols = [col for col in text_columns if col in df.columns]

    if available_text_cols:
        # Analyze high performers
        high_df = df[df['is_high_performer']].copy()
        low_df = df[df['expected_performance'] == 'low'].copy()

        if len(high_df) > 0 and len(low_df) > 0:
            # Get words from high performers
            high_texts = []
            for col in available_text_cols:
                texts = high_df[col].fillna('').astype(str).tolist()
                high_texts.extend(texts)
            high_text = ' '.join(high_texts)
            high_cleaned = re.sub(r'[^a-z\s]', ' ', high_text.lower())
            high_words = [w for w in high_cleaned.split() if w not in stop_words and len(w) > 2]
            high_counts = Counter(high_words)

            # Get words from low performers
            low_texts = []
            for col in available_text_cols:
                texts = low_df[col].fillna('').astype(str).tolist()
                low_texts.extend(texts)
            low_text = ' '.join(low_texts)
            low_cleaned = re.sub(r'[^a-z\s]', ' ', low_text.lower())
            low_words = [w for w in low_cleaned.split() if w not in stop_words and len(w) > 2]
            low_counts = Counter(low_words)

            print("\n### Theme Comparison: High vs Low Performers\n")

            # Find distinctive high performer themes
            print("üèÜ THEMES MUCH MORE COMMON IN HIGH PERFORMERS:\n")
            print(f"{'Theme':<20} {'High':<10} {'Low':<10} {'Difference':<12} {'Impact'}")
            print("-" * 70)

            distinctive = []
            for word, high_count in high_counts.most_common(40):
                low_count = low_counts.get(word, 0)
                # Normalize by number of videos
                high_rate = high_count / len(high_df)
                low_rate = low_count / len(low_df) if len(low_df) > 0 else 0.01
                ratio = high_rate / low_rate if low_rate > 0 else high_rate

                if ratio > 1.5 and high_count > 10:  # Significant difference
                    distinctive.append((word, high_count, low_count, ratio))

            distinctive.sort(key=lambda x: x[3], reverse=True)

            for word, high_c, low_c, ratio in distinctive[:15]:
                impact = "üî•üî•üî•" if ratio > 3 else "üî•üî•" if ratio > 2 else "üî•"
                print(f"{word:<20} {high_c:<10} {low_c:<10} {ratio:<12.1f}x {impact}")

            # Visualize
            fig, axes = plt.subplots(2, 1, figsize=(14, 10))

            # Top themes in high performers
            top_high = high_counts.most_common(12)
            words_high, counts_high = zip(*top_high)
            colors_gradient = plt.cm.viridis(np.linspace(0.3, 0.9, len(words_high)))

            bars = axes[0].barh(words_high, counts_high, color=colors_gradient)
            axes[0].set_xlabel('Frequency', fontweight='bold', fontsize=11)
            axes[0].set_title('Top 12 Themes in HIGH-Performing Videos',
                            fontweight='bold', fontsize=13, color='#2ecc71')
            axes[0].invert_yaxis()
            axes[0].grid(axis='x', alpha=0.3)

            for bar in bars:
                width = bar.get_width()
                axes[0].text(width, bar.get_y() + bar.get_height()/2,
                           f' {int(width)}', va='center', fontweight='bold', fontsize=9)

            # Distinctive themes (high vs low ratio)
            distinctive_top = distinctive[:10]
            words_dist, counts_h, counts_l, ratios = zip(*distinctive_top)

            x = np.arange(len(words_dist))
            width = 0.35

            bars1 = axes[1].bar(x - width/2, counts_h, width, label='High Performers',
                              color='#2ecc71', alpha=0.8)
            bars2 = axes[1].bar(x + width/2, counts_l, width, label='Low Performers',
                              color='#e74c3c', alpha=0.8)

            axes[1].set_ylabel('Frequency', fontweight='bold', fontsize=11)
            axes[1].set_title('Most Distinctive Themes: High vs Low Performers',
                            fontweight='bold', fontsize=13)
            axes[1].set_xticks(x)
            axes[1].set_xticklabels(words_dist, rotation=45, ha='right')
            axes[1].legend()
            axes[1].grid(axis='y', alpha=0.3)

            plt.tight_layout()
            plt.savefig('theme_analysis.png', dpi=300, bbox_inches='tight')
            print("\n‚úì Saved: theme_analysis.png")
            plt.close()

            # Key themes to focus on
            print("\n\nüí° KEY THEMES TO EMPHASIZE:")
            for i, (word, high_c, low_c, ratio) in enumerate(distinctive[:5], 1):
                print(f"   {i}. '{word.upper()}' - appears {ratio:.1f}x more in high performers")

    # --- FINAL RECOMMENDATIONS ---
    print("\n\n" + "="*80)
    print("## üéØ ACTIONABLE RECOMMENDATIONS FOR SPONSORED VIDEO SUCCESS")
    print("="*80)

    print("\n1Ô∏è‚É£  SCORE TARGETS:")
    if all(col in df.columns for col in ['authenticity_score', 'shareability_score']):
        high_auth_avg = df[df['is_high_performer']]['authenticity_score'].mean()
        high_share_avg = df[df['is_high_performer']]['shareability_score'].mean()
        print(f"   ‚Ä¢ Target Authenticity Score: {high_auth_avg:.1f}+ (high performers average {high_auth_avg:.2f})")
        print(f"   ‚Ä¢ Target Shareability Score: {high_share_avg:.1f}+ (high performers average {high_share_avg:.2f})")
        print(f"   ‚Ä¢ PRIORITY: Shareability matters MORE than authenticity for engagement")

    print("\n2Ô∏è‚É£  CONTENT STRATEGY:")
    if 'performance_drivers' in df.columns and len(strategy_df) > 0:
        top_strategies = strategy_df.head(3)['Strategy'].tolist()
        print(f"   ‚Ä¢ Focus on: {', '.join(top_strategies)}")
        print(f"   ‚Ä¢ These strategies are {strategy_df.iloc[0]['Impact_Ratio']:.1f}x more common in high performers")

    print("\n3Ô∏è‚É£  CONTENT THEMES:")
    if available_text_cols and len(distinctive) > 0:
        top_themes = [w for w, _, _, _ in distinctive[:3]]
        print(f"   ‚Ä¢ Emphasize: {', '.join(top_themes)}")
        print(f"   ‚Ä¢ These themes strongly differentiate successful content")

    print("\n4Ô∏è‚É£  AVOID:")
    print(f"   ‚Ä¢ Low authenticity (<3) + Low shareability (<3) = {success_rate.iloc[0, 0]:.1f}% success rate")
    print(f"   ‚Ä¢ This is {best_rate / success_rate.iloc[0, 0]:.1f}x worse than the best combination")

    print("\n\n" + "="*80)
    print("                     ANALYSIS COMPLETE")
    print("="*80)
    print("\nüìä Generated visualizations:")
    print("  1. score_combination_analysis.png - Which score combos work best")
    print("  2. content_strategy_effectiveness.png - Which strategies drive performance")
    print("  3. theme_analysis.png - What high performers talk about")
    print("\n" + "="*80 + "\n")


if __name__ == "__main__":
    analyze_video_performance()

