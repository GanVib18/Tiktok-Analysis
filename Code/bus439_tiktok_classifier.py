# -*- coding: utf-8 -*-
"""BUS439_TikTok_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yJqO1NWVFJSxF-ejixDUQkFm5avfYEr7
"""

import pandas as pd
import re
from typing import Tuple, List, Dict
import numpy as np


class SponsoredDetector:
    """
    Enhanced sponsored content detector with transcript analysis.
    Analyzes TikTok videos for sponsored content using multi-signal detection.
    """

    # Scoring weights for different signals (aggressive)
    SCORES = {
        'explicit': 85,
        'implicit_multi': 65,
        'implicit_single': 40,
        'promo': 70,
        'brand': 30,
        'cta': 25,
        'engagement': 20,
        'transcript_disclosure': 60,
        'transcript_promo': 50,
        'product_language': 25,
        'multiple_brands': 35,
        'review_language': 30,
    }

    def __init__(self):
        """Initialize detection patterns and keywords."""
        # Explicit disclosure hashtags
        self.disclosure_explicit = [
            '#ad', '#sponsored', '#paidpartnership', '#spon',
            '#sponsoredcontent', '#paidpromotion', '#advertisement'
        ]

        # Implicit disclosure keywords (expanded)
        self.disclosure_implicit = [
            'partner', 'partnership', 'collab', 'collaboration',
            'ambassador', 'paid', 'gifted', 'affiliate', 'promo',
            'sponsored by', 'in partnership with', 'thanks to',
            'working with', 'teamed up', 'brought to you by',
            'pr', 'press', 'sent', 'provided by'
        ]

        # Brand-related patterns
        self.brand_patterns = [
            r'#\w+partner',
            r'#\w+ambassador',
            r'#\w+collab',
            r'@\w+official',
            r'@\w+brand'
        ]

        # Call-to-action keywords
        self.cta_keywords = {
            'purchase': ['shop now', 'buy now', 'get yours', 'order now', 'purchase', 'grab yours'],
            'discount': ['discount', 'promo', 'code', 'coupon', 'off', 'sale', '% off', 'percent off'],
            'link': ['link in bio', 'swipe up', 'click link', 'check out', 'tap the link'],
            'urgency': ['limited time', 'today only', 'hurry', 'dont miss', "don't miss", 'act now']
        }

        # Transcript-specific patterns
        self.transcript_disclosure = [
            r'\b(?:this\s+(?:video|post)\s+is\s+)?sponsored\s+by\b',
            r'\b(?:thanks|thank\s+you)\s+to\s+\w+\s+for\s+sponsoring\b',
            r'\bpaid\s+partnership\s+with\b',
            r'\bin\s+partnership\s+with\b',
            r'\bproudly\s+partnered\s+with\b'
        ]

    def _combine_text(self, row: pd.Series) -> str:
        """Combine text fields (excluding transcript for separate analysis)."""
        fields = ['title', 'description', 'hashtags']
        text_parts = [str(row[col]).lower() for col in fields
                     if col in row and pd.notna(row[col]) and str(row[col]).lower() != 'nan']
        return ' '.join(text_parts)

    def _get_transcript(self, row: pd.Series) -> str:
        """Get transcript text safely."""
        if 'transcript_text' in row and pd.notna(row['transcript_text']):
            return str(row['transcript_text']).lower()
        return ''

    def _detect_explicit(self, text: str) -> Tuple[int, bool]:
        """Detect explicit disclosure hashtags."""
        found = any(kw in text for kw in self.disclosure_explicit)
        return (self.SCORES['explicit'], found) if found else (0, False)

    def _detect_implicit(self, text: str) -> Tuple[int, bool]:
        """Detect implicit disclosure language with boundary matching."""
        count = sum(1 for kw in self.disclosure_implicit
                   if re.search(r'\b' + re.escape(kw) + r'\b', text, re.I))

        if count >= 2:
            return self.SCORES['implicit_multi'], True
        elif count == 1:
            return self.SCORES['implicit_single'], True
        return 0, False

    def _detect_promo(self, text: str) -> Tuple[int, bool]:
        """Detect promotional codes and discount offers."""
        patterns = [
            r'\b(?:code|promo|coupon)[\s:]+["\']?([A-Z0-9]{3,15})',
            r'\buse\s+code\s+[A-Z0-9]{3,15}\b',
            r'\b\d+%\s*off\b',
            r'\bsave\s+\d+%\b'
        ]
        found = any(re.search(p, text, re.I) for p in patterns)
        return (self.SCORES['promo'], found) if found else (0, False)

    def _detect_brands(self, text: str) -> Tuple[int, List[str]]:
        """Detect brand mention patterns."""
        brands = []
        for pattern in self.brand_patterns:
            matches = re.findall(pattern, text, re.I)
            brands.extend(matches)

        # Cap scoring at 40 points max
        score = min(len(brands) * 10, 40)
        return score, list(set(brands))[:5]  # Return top 5 unique brands

    def _detect_cta(self, text: str) -> Tuple[int, List[str]]:
        """Detect call-to-action patterns."""
        found_types = []
        for cta_type, phrases in self.cta_keywords.items():
            if any(phrase in text for phrase in phrases):
                found_types.append(cta_type)

        # Cap at 35 points
        score = min(len(found_types) * self.SCORES['cta'], 35)
        return score, found_types

    def _analyze_transcript(self, transcript: str) -> Tuple[int, List[str]]:
        """Analyze transcript for sponsored content indicators."""
        if not transcript:
            return 0, []

        score = 0
        indicators = []

        # Check for explicit disclosure patterns
        for pattern in self.transcript_disclosure:
            if re.search(pattern, transcript, re.I):
                score += self.SCORES['transcript_disclosure']
                indicators.append('transcript_disclosure')
                break

        # Check for promo codes mentioned verbally
        promo_patterns = [
            r'\buse\s+(?:code|promo)\s+[A-Z0-9]{3,15}\b',
            r'\benter\s+code\s+[A-Z0-9]{3,15}\b',
            r'\btype\s+in\s+[A-Z0-9]{3,15}\b'
        ]
        if any(re.search(p, transcript, re.I) for p in promo_patterns):
            score += self.SCORES['transcript_promo']
            indicators.append('transcript_promo')

        # Check for product recommendations
        product_language = [
            r'\bi\s+(?:love|recommend|suggest|use)\s+(?:this|these|the)\b',
            r'\byou\s+(?:should|need\s+to|have\s+to|must)\s+(?:try|get|buy|check\s+out)\b',
            r'\bcheck\s+out\s+(?:this|these|the)\b',
            r'\bmy\s+favorite\b',
            r'\bobsessed\s+with\b',
            r'\bgame\s+changer\b',
            r'\bholy\s+grail\b',
            r'\b(?:best|favorite)\s+product\b'
        ]
        if any(re.search(p, transcript, re.I) for p in product_language):
            score += self.SCORES['product_language']
            indicators.append('product_recommendation')

        return score, indicators

    def _check_engagement(self, row: pd.Series, df: pd.DataFrame) -> int:
        """Check for anomalously high engagement rate."""
        required_cols = ['view_count', 'like_count']
        if not all(col in row.index for col in required_cols):
            return 0

        if pd.isna(row['view_count']) or pd.isna(row['like_count']) or row['view_count'] <= 0:
            return 0

        engagement_rate = (row['like_count'] / row['view_count']) * 100

        # Calculate threshold from dataset
        valid_df = df[(df['view_count'].notna()) &
                     (df['like_count'].notna()) &
                     (df['view_count'] > 0)]

        if len(valid_df) < 5:  # Need minimum sample size
            return 0

        eng_rates = (valid_df['like_count'] / valid_df['view_count']) * 100
        threshold = eng_rates.quantile(0.80)  # 80th percentile (more generous)

        return self.SCORES['engagement'] if engagement_rate > threshold else 0

    def _analyze_row(self, row: pd.Series, df: pd.DataFrame) -> pd.Series:
        """Perform comprehensive analysis on a single video."""
        text = row['combined_text']
        transcript = row['transcript']

        score = 0
        methods = []
        indicators = []

        # 1. Explicit disclosure
        s, found = self._detect_explicit(text)
        if found:
            score += s
            methods.append('explicit_disclosure')
            indicators.append(f'Explicit disclosure ({s}pts)')

        # 2. Implicit disclosure
        s, found = self._detect_implicit(text)
        if found:
            score += s
            methods.append('implicit_disclosure')
            indicators.append(f'Implicit disclosure ({s}pts)')

        # 3. Promotional code
        s, found = self._detect_promo(text)
        if found:
            score += s
            methods.append('promo_code')
            indicators.append(f'Promo code ({s}pts)')

        # 4. Brand patterns
        s, brands = self._detect_brands(text)
        if s > 0:
            score += s
            methods.append('brand_pattern')
            indicators.append(f'Brand patterns ({s}pts)')
            row['brand_mentions'] = ', '.join(brands)

        # 5. Call-to-action
        s, cta_types = self._detect_cta(text)
        if s > 0:
            score += s
            methods.append('cta_pattern')
            indicators.append(f'CTA: {", ".join(cta_types)} ({s}pts)')

        # 6. Transcript analysis (NEW)
        s, trans_indicators = self._analyze_transcript(transcript)
        if s > 0:
            score += s
            methods.extend(trans_indicators)
            indicators.append(f'Transcript signals ({s}pts)')

        # 7. Engagement anomaly
        s = self._check_engagement(row, df)
        if s > 0:
            score += s
            methods.append('engagement_anomaly')
            indicators.append(f'High engagement ({s}pts)')

        # Determine classification
        confidence = min(score, 100)

        # More generous thresholds
        if 'explicit_disclosure' in methods:
            threshold = 40  # Very likely if explicit disclosure
        elif 'transcript_disclosure' in methods:
            threshold = 45  # Likely if disclosed in transcript
        else:
            threshold = 48  # Need more signals without disclosure

        row['is_sponsored'] = confidence >= threshold
        row['confidence_score'] = round(confidence, 2)
        row['detection_method'] = ', '.join(methods) if methods else 'none'
        row['sponsored_indicators'] = ' | '.join(indicators) if indicators else 'none'

        return row

    def detect(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Run sponsored content detection on entire dataframe.

        Args:
            df: DataFrame with TikTok video data

        Returns:
            DataFrame with added detection columns
        """
        if df.empty:
            print("âš ï¸  Cannot run detection on empty dataframe")
            return df

        print(f"\n{'='*60}")
        print(f"SPONSORED CONTENT DETECTION")
        print(f"{'='*60}")
        print(f"Analyzing {len(df):,} videos...")

        df = df.copy()

        # Prepare text fields
        df['combined_text'] = df.apply(self._combine_text, axis=1)
        df['transcript'] = df.apply(self._get_transcript, axis=1)

        # Initialize result columns
        df['is_sponsored'] = False
        df['confidence_score'] = 0.0
        df['detection_method'] = ''
        df['sponsored_indicators'] = ''
        df['brand_mentions'] = ''

        # Analyze each row
        df = df.apply(lambda row: self._analyze_row(row, df), axis=1)

        # Cleanup temporary columns
        df.drop(columns=['combined_text', 'transcript'], inplace=True)

        # Summary statistics
        sponsored_count = df['is_sponsored'].sum()
        sponsored_pct = (sponsored_count / len(df)) * 100
        avg_confidence = df[df['is_sponsored']]['confidence_score'].mean()

        print(f"\n{'='*60}")
        print(f"RESULTS SUMMARY")
        print(f"{'='*60}")
        print(f"Total videos analyzed: {len(df):,}")
        print(f"Sponsored content detected: {sponsored_count:,} ({sponsored_pct:.1f}%)")
        if sponsored_count > 0:
            print(f"Average confidence score: {avg_confidence:.1f}%")
        print(f"{'='*60}\n")

        return df


def analyze_tiktok_data(csv_path: str) -> pd.DataFrame:
    """
    Main function to load and analyze TikTok data for sponsored content.

    Args:
        csv_path: Path to CSV file with TikTok data

    Returns:
        DataFrame with detection results
    """
    print(f"Loading data from: {csv_path}")
    df = pd.read_csv(csv_path)

    print(f"âœ“ Loaded {len(df):,} records")
    print(f"âœ“ Columns: {len(df.columns)}")

    # Run detection
    detector = SponsoredDetector()
    results = detector.detect(df)

    return results


# Example usage
if __name__ == "__main__":
    # Run analysis
    results_df = analyze_tiktok_data('/content/tiktok_data.csv')

    # Display sample results
    sponsored_videos = results_df[results_df['is_sponsored']]
    print("\nSample of detected sponsored content:")
    print(sponsored_videos[['title', 'confidence_score', 'detection_method']].head(10))

    # Optional: Save results
    output_path = '/content/tiktok_data_analyzed.csv'
    results_df.to_csv(output_path, index=False)
    print(f"\nâœ“ Results saved to: {output_path}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    roc_auc_score, roc_curve, precision_recall_curve,
    confusion_matrix, classification_report, auc
)
from typing import Optional, Dict, Tuple
import warnings
warnings.filterwarnings('ignore')


class DetectorEvaluator:
    """
    Evaluate sponsored content detector performance.
    Supports both supervised (with labels) and unsupervised analysis.
    """

    def __init__(self, results_df: pd.DataFrame, ground_truth_col: Optional[str] = None):
        """
        Initialize evaluator.

        Args:
            results_df: DataFrame with detection results
            ground_truth_col: Name of column with true labels (if available)
        """
        self.df = results_df.copy()
        self.ground_truth_col = ground_truth_col
        self.has_labels = ground_truth_col and ground_truth_col in results_df.columns

    def supervised_metrics(self) -> Dict:
        """Calculate metrics when ground truth is available."""
        if not self.has_labels:
            print("âš ï¸  No ground truth labels provided. Use unsupervised_analysis() instead.")
            return {}

        y_true = self.df[self.ground_truth_col].astype(bool)
        y_pred = self.df['is_sponsored'].astype(bool)
        y_score = self.df['confidence_score'] / 100  # Normalize to 0-1

        # Basic metrics
        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()

        metrics = {
            'accuracy': (tp + tn) / (tp + tn + fp + fn),
            'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,
            'recall': tp / (tp + fn) if (tp + fn) > 0 else 0,
            'f1_score': 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0,
            'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0,
            'false_positive_rate': fp / (fp + tn) if (fp + tn) > 0 else 0,
            'false_negative_rate': fn / (fn + tp) if (fn + tp) > 0 else 0,
            'true_positives': int(tp),
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
        }

        # ROC-AUC
        try:
            metrics['roc_auc'] = roc_auc_score(y_true, y_score)
        except:
            metrics['roc_auc'] = None

        # PR-AUC
        try:
            precision, recall, _ = precision_recall_curve(y_true, y_score)
            metrics['pr_auc'] = auc(recall, precision)
        except:
            metrics['pr_auc'] = None

        return metrics

    def print_supervised_report(self):
        """Print comprehensive evaluation report with ground truth."""
        if not self.has_labels:
            print("âš ï¸  No ground truth labels available")
            return

        metrics = self.supervised_metrics()

        print("\n" + "="*70)
        print("SUPERVISED EVALUATION REPORT")
        print("="*70)

        print(f"\nðŸ“Š CONFUSION MATRIX")
        print(f"{'':20} Predicted Negative    Predicted Positive")
        print(f"Actual Negative      {metrics['true_negatives']:8d}           {metrics['false_positives']:8d}")
        print(f"Actual Positive      {metrics['false_negatives']:8d}           {metrics['true_positives']:8d}")

        print(f"\nðŸŽ¯ CLASSIFICATION METRICS")
        print(f"Accuracy:            {metrics['accuracy']:.3f}")
        print(f"Precision:           {metrics['precision']:.3f}")
        print(f"Recall (Sensitivity): {metrics['recall']:.3f}")
        print(f"F1 Score:            {metrics['f1_score']:.3f}")
        print(f"Specificity:         {metrics['specificity']:.3f}")

        print(f"\nâš ï¸  ERROR RATES")
        print(f"False Positive Rate: {metrics['false_positive_rate']:.3f} ({metrics['false_positives']} videos)")
        print(f"False Negative Rate: {metrics['false_negative_rate']:.3f} ({metrics['false_negatives']} videos)")

        if metrics['roc_auc']:
            print(f"\nðŸ“ˆ CURVE METRICS")
            print(f"ROC-AUC:             {metrics['roc_auc']:.3f}")
        if metrics['pr_auc']:
            print(f"PR-AUC:              {metrics['pr_auc']:.3f}")

        print("\n" + "="*70)

        # Sklearn classification report
        y_true = self.df[self.ground_truth_col].astype(bool)
        y_pred = self.df['is_sponsored'].astype(bool)
        print("\n" + classification_report(y_true, y_pred,
                                          target_names=['Not Sponsored', 'Sponsored']))

    def plot_supervised_curves(self, save_path: Optional[str] = None):
        """Plot ROC and PR curves."""
        if not self.has_labels:
            print("âš ï¸  No ground truth labels available")
            return

        y_true = self.df[self.ground_truth_col].astype(bool)
        y_score = self.df['confidence_score'] / 100

        fig, axes = plt.subplots(1, 2, figsize=(14, 5))

        # ROC Curve
        fpr, tpr, _ = roc_curve(y_true, y_score)
        roc_auc = auc(fpr, tpr)

        axes[0].plot(fpr, tpr, color='darkorange', lw=2,
                    label=f'ROC curve (AUC = {roc_auc:.3f})')
        axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
        axes[0].set_xlim([0.0, 1.0])
        axes[0].set_ylim([0.0, 1.05])
        axes[0].set_xlabel('False Positive Rate')
        axes[0].set_ylabel('True Positive Rate')
        axes[0].set_title('ROC Curve')
        axes[0].legend(loc="lower right")
        axes[0].grid(alpha=0.3)

        # Precision-Recall Curve
        precision, recall, _ = precision_recall_curve(y_true, y_score)
        pr_auc = auc(recall, precision)

        axes[1].plot(recall, precision, color='green', lw=2,
                    label=f'PR curve (AUC = {pr_auc:.3f})')
        axes[1].set_xlim([0.0, 1.0])
        axes[1].set_ylim([0.0, 1.05])
        axes[1].set_xlabel('Recall')
        axes[1].set_ylabel('Precision')
        axes[1].set_title('Precision-Recall Curve')
        axes[1].legend(loc="lower left")
        axes[1].grid(alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ“ Curves saved to {save_path}")

        plt.show()

    def unsupervised_analysis(self) -> Dict:
        """Analyze detection patterns without ground truth."""
        detected = self.df[self.df['is_sponsored']]
        not_detected = self.df[~self.df['is_sponsored']]

        analysis = {
            'total_videos': len(self.df),
            'detected_sponsored': len(detected),
            'detection_rate': len(detected) / len(self.df) * 100,
            'avg_confidence_detected': detected['confidence_score'].mean() if len(detected) > 0 else 0,
            'median_confidence_detected': detected['confidence_score'].median() if len(detected) > 0 else 0,
            'avg_confidence_not_detected': not_detected['confidence_score'].mean() if len(not_detected) > 0 else 0,
        }

        # Method frequency
        if len(detected) > 0:
            all_methods = detected['detection_method'].str.split(', ').explode()
            analysis['method_distribution'] = all_methods.value_counts().to_dict()

        # Confidence distribution
        analysis['confidence_bins'] = {
            '0-25': len(self.df[self.df['confidence_score'] < 25]),
            '25-50': len(self.df[(self.df['confidence_score'] >= 25) & (self.df['confidence_score'] < 50)]),
            '50-75': len(self.df[(self.df['confidence_score'] >= 50) & (self.df['confidence_score'] < 75)]),
            '75-100': len(self.df[self.df['confidence_score'] >= 75]),
        }

        return analysis

    def print_unsupervised_report(self):
        """Print analysis report without ground truth."""
        analysis = self.unsupervised_analysis()

        print("\n" + "="*70)
        print("UNSUPERVISED ANALYSIS REPORT")
        print("="*70)

        print(f"\nðŸ“Š DETECTION SUMMARY")
        print(f"Total videos:           {analysis['total_videos']:,}")
        print(f"Detected sponsored:     {analysis['detected_sponsored']:,} ({analysis['detection_rate']:.1f}%)")
        print(f"Detected non-sponsored: {analysis['total_videos'] - analysis['detected_sponsored']:,} ({100-analysis['detection_rate']:.1f}%)")

        print(f"\nðŸŽ¯ CONFIDENCE SCORES")
        print(f"Avg (detected):         {analysis['avg_confidence_detected']:.1f}%")
        print(f"Median (detected):      {analysis['median_confidence_detected']:.1f}%")
        print(f"Avg (not detected):     {analysis['avg_confidence_not_detected']:.1f}%")

        print(f"\nðŸ“ˆ CONFIDENCE DISTRIBUTION")
        for bin_range, count in analysis['confidence_bins'].items():
            pct = count / analysis['total_videos'] * 100
            print(f"{bin_range}%:  {count:5d} videos ({pct:5.1f}%)")

        if 'method_distribution' in analysis:
            print(f"\nðŸ” DETECTION METHODS (Top 10)")
            for i, (method, count) in enumerate(list(analysis['method_distribution'].items())[:10], 1):
                print(f"{i:2d}. {method:30s} {count:4d} times")

        print("\n" + "="*70)

    def plot_unsupervised_analysis(self, save_path: Optional[str] = None):
        """Create visualization of detection patterns."""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))

        # 1. Detection distribution
        detected_counts = self.df['is_sponsored'].value_counts()
        axes[0, 0].pie(detected_counts, labels=['Not Sponsored', 'Sponsored'],
                      autopct='%1.1f%%', startangle=90, colors=['#3498db', '#e74c3c'])
        axes[0, 0].set_title('Detection Distribution')

        # 2. Confidence score distribution
        axes[0, 1].hist(self.df[self.df['is_sponsored']]['confidence_score'],
                       bins=20, alpha=0.7, label='Detected', color='#e74c3c')
        axes[0, 1].hist(self.df[~self.df['is_sponsored']]['confidence_score'],
                       bins=20, alpha=0.7, label='Not Detected', color='#3498db')
        axes[0, 1].set_xlabel('Confidence Score')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].set_title('Confidence Score Distribution')
        axes[0, 1].legend()
        axes[0, 1].grid(alpha=0.3)

        # 3. Detection methods
        if len(self.df[self.df['is_sponsored']]) > 0:
            methods = self.df[self.df['is_sponsored']]['detection_method'].str.split(', ').explode()
            method_counts = methods.value_counts().head(10)
            axes[1, 0].barh(range(len(method_counts)), method_counts.values, color='#2ecc71')
            axes[1, 0].set_yticks(range(len(method_counts)))
            axes[1, 0].set_yticklabels(method_counts.index)
            axes[1, 0].set_xlabel('Count')
            axes[1, 0].set_title('Top 10 Detection Methods')
            axes[1, 0].grid(axis='x', alpha=0.3)

        # 4. Confidence vs Engagement (if available)
        if 'view_count' in self.df.columns and 'like_count' in self.df.columns:
            valid_df = self.df[(self.df['view_count'] > 0) & (self.df['like_count'].notna())]
            if len(valid_df) > 0:
                valid_df['engagement_rate'] = (valid_df['like_count'] / valid_df['view_count']) * 100

                sponsored = valid_df[valid_df['is_sponsored']]
                not_sponsored = valid_df[~valid_df['is_sponsored']]

                axes[1, 1].scatter(not_sponsored['engagement_rate'], not_sponsored['confidence_score'],
                                 alpha=0.5, s=20, label='Not Sponsored', color='#3498db')
                axes[1, 1].scatter(sponsored['engagement_rate'], sponsored['confidence_score'],
                                 alpha=0.7, s=30, label='Sponsored', color='#e74c3c')
                axes[1, 1].set_xlabel('Engagement Rate (%)')
                axes[1, 1].set_ylabel('Confidence Score')
                axes[1, 1].set_title('Confidence vs Engagement')
                axes[1, 1].legend()
                axes[1, 1].grid(alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ“ Plots saved to {save_path}")

        plt.show()

    def sample_detection_errors(self, n: int = 10) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Sample potential false positives and false negatives."""
        if not self.has_labels:
            print("âš ï¸  No ground truth labels available")
            return pd.DataFrame(), pd.DataFrame()

        y_true = self.df[self.ground_truth_col].astype(bool)
        y_pred = self.df['is_sponsored'].astype(bool)

        # False positives (predicted sponsored, actually not)
        fp_mask = (~y_true) & (y_pred)
        false_positives = self.df[fp_mask].nlargest(n, 'confidence_score')

        # False negatives (predicted not sponsored, actually sponsored)
        fn_mask = (y_true) & (~y_pred)
        false_negatives = self.df[fn_mask].nlargest(n, 'confidence_score')

        return false_positives, false_negatives


def evaluate_detector(results_df: pd.DataFrame,
                      ground_truth_col: Optional[str] = None,
                      save_plots: bool = False):
    """
    Main evaluation function.

    Args:
        results_df: DataFrame with detection results
        ground_truth_col: Column name with true labels (optional)
        save_plots: Whether to save plots to files
    """
    evaluator = DetectorEvaluator(results_df, ground_truth_col)

    if evaluator.has_labels:
        # Supervised evaluation
        evaluator.print_supervised_report()
        evaluator.plot_supervised_curves(
            save_path='roc_pr_curves.png' if save_plots else None
        )

        # Sample errors
        print("\n" + "="*70)
        print("SAMPLE DETECTION ERRORS")
        print("="*70)

        fp, fn = evaluator.sample_detection_errors(5)

        if len(fp) > 0:
            print("\nðŸ”´ FALSE POSITIVES (Top 5 by confidence)")
            print(fp[['title', 'confidence_score', 'detection_method', 'sponsored_indicators']].to_string())

        if len(fn) > 0:
            print("\nðŸ”µ FALSE NEGATIVES (Top 5 by confidence)")
            print(fn[['title', 'confidence_score', 'detection_method']].to_string())
    else:
        # Unsupervised analysis
        evaluator.print_unsupervised_report()
        evaluator.plot_unsupervised_analysis(
            save_path='detection_analysis.png' if save_plots else None
        )

    return evaluator


# Example usage
if __name__ == "__main__":
    # Load your detection results
    results_df = pd.read_csv('/content/tiktok_data_analyzed.csv')

    # Option 1: If you have ground truth labels
    # evaluator = evaluate_detector(results_df, ground_truth_col='true_label', save_plots=True)

    # Option 2: Without ground truth (unsupervised analysis)
    evaluator = evaluate_detector(results_df, ground_truth_col=None, save_plots=True)

